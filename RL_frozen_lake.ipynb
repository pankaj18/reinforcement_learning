{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_frozen_lake.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnbn/4zoMiFMJxDFKs6Kk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankaj18/reinforcement_learning/blob/master/RL_frozen_lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgheNMHh65l0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "ccd99516-38ae-4a1f-c6b7-0123b6a89ba1"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 3s (192 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n",
            "Fetched 784 kB in 3s (292 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 146734 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaknnFdAC934",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "This is the hero of our story, the gumdrop emoji. It was enjoying a cool winter day building a snowman when suddely, it slipped and fell on a frozen lake of death.\n",
        "\n",
        "The lake can be thought of as a 4 x 4 grid where the gumdrop can move left (0), down (1), right (2) and up (3). Unfortunately, this frozen lake of death has holes of death where if the gumdrop enters that square, it will fall in and meet an untimely demise. To make matters worse, the lake is surrounded by icy boulders that if the gumdrop attempts to climb, will have it slip back into its original position. Thankfully, at the bottom right of the lake is a safe ramp that leads to a nice warm cup of hot cocoa.\n",
        "\n",
        "## Set Up\n",
        "\n",
        "We can try and save the gumdrop ourselves! This is a common game people begin their Reinforcement Learning journey with, and is included in the OpenAI's python package [Gym](https://gym.openai.com/) and is aptly named [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) ([code](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py)). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PAtI8AL_QwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('dark_background')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FR_3BQBbEC",
        "colab_type": "text"
      },
      "source": [
        "There are [four methods from Gym](http://gym.openai.com/docs/) that are going to be useful to us in order to save the gumdrop.\n",
        "* `make` allows us to build the environment or game that we can pass actions to\n",
        "* `reset` will reset an environment to it's starting configuration and return the state of the player\n",
        "* `render` displays the environment for human eyes\n",
        "* `step` takes an action and returns the player's next state.\n",
        "\n",
        "Let's make, reset, and render the game. The output is an ANSI string with the following characters:\n",
        "* `S` for starting point\n",
        "* `F` for frozen\n",
        "* `H` for hole\n",
        "* `G` for goal\n",
        "* A red square indicates the current position"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz6Yom2S_mIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "da68bf88-3057-4007-c551-d24d5b0d4985"
      },
      "source": [
        "env=gym.make('FrozenLake-v0',is_slippery=False)\n",
        "state=env.reset()\n",
        "env.render()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZUT3uwCDnvt",
        "colab_type": "text"
      },
      "source": [
        "If we print the state we'll get `0`. This is telling us which square we're in. Each square is labeled from `0` to `15` from left to right, top to bottom, like this:\n",
        "\n",
        "| | | | |\n",
        "|-|-|-|-|\n",
        "|0|1|2|3|\n",
        "|4|5|6|7|\n",
        "|8|9|10|11|\n",
        "|12|13|14|15|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYCe8YhHB4ST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "330dbb8b-40aa-4881-dc05-4a9737d1d764"
      },
      "source": [
        "print(state)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjDwvSEXDjSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_state(state, done):\n",
        "    statement = \"Still Alive!\"\n",
        "    if done:\n",
        "        statement = \"Cocoa Time!\" if state == 15 else \"Game Over!\" \n",
        "    print(state, \"-\", statement)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAZc6oyXEE_f",
        "colab_type": "text"
      },
      "source": [
        "We can control the gumdrop ourselves with the `step` method. Run the below cell over and over again trying to move from the starting position to the goal. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8qoAFc8D69c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "289fcfc3-4c1b-4dac-f758-5d0dd5ac614e"
      },
      "source": [
        "#0 left\n",
        "#1 down\n",
        "#2 right\n",
        "#3 up\n",
        "\n",
        "# Uncomment to reset the game\n",
        "env.reset()\n",
        "action=2\n",
        "state,r,done,info=env.step(action)\n",
        "env.render()\n",
        "print_state(state,done)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "1 - Still Alive!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjSzVBUwEx_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "5cd65576-6ff6-4a64-b9fb-8226f6e8b97d"
      },
      "source": [
        "def play_game(actions):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    while not done and step < len(actions):\n",
        "        action = actions[step]\n",
        "        state, _, done, _ = env.step(action)\n",
        "        env.render()\n",
        "        step += 1\n",
        "        print_state(state, done)\n",
        "        \n",
        "actions = [1, 1, 2, 2, 1, 2]  # Replace with your favorite path.\n",
        "play_game(actions)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "8 - Still Alive!\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "9 - Still Alive!\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "10 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "14 - Still Alive!\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "15 - Cocoa Time!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N_Uy_elJAUt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Value Iteration\n",
        "\n",
        "Let's turn the clock back on our time machines to 1957 to meet Mr. [Richard Bellman](https://en.wikipedia.org/wiki/Richard_E._Bellman). Bellman started his academic career in mathematics, but due to World War II, left his postgraduate studies at John Hopkins to teach electronics as part of the war effort (as chronicled by J. J. O'Connor and E. F. Robertson [here](https://www-history.mcs.st-andrews.ac.uk/Biographies/Bellman.html)). When the war was over, and it came time for him to focus on his next area of research, he became fascinated with [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming): the idea of breaking a problem down into sub-problems and using recursion to solve the larger problem.\n",
        "\n",
        "Eventually, his research landed him on [Markov Decision Processes](https://en.wikipedia.org/wiki/Markov_decision_process). These processes are a graphical way of representing how to make a decision based on a current state. States are connected to other states with positive and negative rewards that can be picked up along the way.\n",
        "\n",
        "Sound familiar at all? Perhaps our Frozen Lake?\n",
        "\n",
        "In the lake case, each cell is a state. The `H`s and the `G` are a special type of state called a \"Terminal State\", meaning they can be entered, but they have no leaving connections. What of rewards? Let's say the value of losing our life is the negative opposite of getting to the goal and staying alive. Thus, we can assign the reward of entering a death hole as -1, and the reward of escaping as +1.\n",
        "\n",
        "Bellman's first breakthrough with this type of problem is now known as Value Iteration ([his original paper](http://www.iumj.indiana.edu/IUMJ/FULLTEXT/1957/6/56038)). He introduced a variable, gamma (γ), to represent discounted future rewards. He also introduced a function of policy (π) that takes a state (s), and outputs corresponding suggested action (a). The goal is to find the value of a state (V), given the rewards that occur when following an action in a particular state (R).\n",
        "\n",
        "Gamma, the discount, is the key ingredient here. If my time steps were in days, and my gamma was .9, `$100` would be worth `$100` to me today, `$90` tomorrow, `$81` the day after, and so on. Putting this all together, we get the Bellman Equation\n",
        "\n",
        "\n",
        "source: [Wikipedia](https://en.wikipedia.org/wiki/Bellman_equation)\n",
        "\n",
        "In other words, the value of our current state, `current_values`, is equal to the discount times the value of the next state, `next_values`, given the policy the agent will follow. For now, we'll have our agent assume a greedy policy: it will move towards the state with the highest calculated value. If you're wondering what P is, don't worry, we'll get to that later.\n",
        "\n",
        "Let's program it out and see it in action! We'll set up an array representing the lake with -1 as the holes, and 1 as the goal. Then, we'll set up an array of zeros to start our iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X5F726vGcdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LAKE = np.array([[0,  0,  0,  0],\n",
        "                 [0, -1,  0, -1],\n",
        "                 [0,  0,  0, -1],\n",
        "                 [-1, 0,  0,  1]])\n",
        "LAKE_WIDTH = len(LAKE[0])\n",
        "LAKE_HEIGHT = len(LAKE)\n",
        "\n",
        "DISCOUNT = .9  # Change me to be a value between 0 and 1.\n",
        "current_values = np.zeros_like(LAKE)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuXrZbOzK3Iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9a885047-8c73-4de9-d98b-a398d91cac2e"
      },
      "source": [
        "print(LAKE_WIDTH)\n",
        "print(LAKE_HEIGHT)\n",
        "print(current_values)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "4\n",
            "[[0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXgJRJocLRw8",
        "colab_type": "text"
      },
      "source": [
        "The Gym environment class has a handy property for finding the number of states in an environment called `observation_space`. In our case, there a 16 integer states, so it will label it as \"Discrete\". Similarly, `action_space` will tell us how many actions are abailable to the agent.\n",
        "\n",
        "Let's take advantage of these to make our code portable between different lakes sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgtJUQGhK6NO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1850f4cc-278a-4769-8de1-01a14eee174d"
      },
      "source": [
        "print(\"env.observation_space -\", env.observation_space)\n",
        "print(\"env.observation_space.n -\", env.observation_space.n)\n",
        "print(\"env.action_space -\", env.action_space)\n",
        "print(\"env.action_space.n -\", env.action_space.n)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env.observation_space - Discrete(16)\n",
            "env.observation_space.n - 16\n",
            "env.action_space - Discrete(4)\n",
            "env.action_space.n - 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC3diXK5Ld64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STATE_SPACE = env.observation_space.n\n",
        "ACTION_SPACE = env.action_space.n\n",
        "STATE_RANGE = range(STATE_SPACE)\n",
        "ACTION_RANGE = range(ACTION_SPACE)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU2CEtk_MAZA",
        "colab_type": "text"
      },
      "source": [
        "We'll need some sort of function to figure out what the best neighboring cell is. The below function take's a cell of the lake, and looks at the current value mapping (to be called with current_values, and see's what the value of the adjacent state is corresponding to the given action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVal32dVLmma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_neighbor_value(state_x, state_y, values, action):\n",
        "    \"\"\"Returns the value of a state's neighbor.\n",
        "    \n",
        "    Args:\n",
        "        state_x (int): The state's horizontal position, 0 is the lake's left.\n",
        "        state_y (int): The state's vertical position, 0 is the lake's top.\n",
        "        values (float array): The current iteration's state values.\n",
        "        policy (int): Which action to check the value for.\n",
        "        \n",
        "    Returns:\n",
        "        The corresponding action's value.\n",
        "    \"\"\"\n",
        "    left = [state_y, state_x-1]\n",
        "    down = [state_y+1, state_x]\n",
        "    right = [state_y, state_x+1]\n",
        "    up = [state_y-1, state_x]\n",
        "    actions = [left, down, right, up]\n",
        "\n",
        "    direction = actions[action]\n",
        "    check_x = direction[1]\n",
        "    check_y = direction[0]\n",
        "        \n",
        "    is_boulder = check_y < 0 or check_y >= LAKE_HEIGHT \\\n",
        "        or check_x < 0 or check_x >= LAKE_WIDTH\n",
        "    \n",
        "    value = values[state_y, state_x]\n",
        "    if not is_boulder:\n",
        "        value = values[check_y, check_x]\n",
        "        \n",
        "    return value"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJwaauxmOGXk",
        "colab_type": "text"
      },
      "source": [
        "But this doesn't find the best action, and the gumdrop is going to need that if it wants to greedily get off the lake. The get_max_neighbor function we've defined below takes a number corresponding to a cell as state_number and the same value mapping as get_neighbor_value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWe5O2JeN2GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_state_coordinates(state_number):\n",
        "    state_x = state_number % LAKE_WIDTH\n",
        "    state_y = state_number // LAKE_HEIGHT\n",
        "    return state_x, state_y\n",
        "\n",
        "def get_max_neighbor(state_number, values):\n",
        "    \"\"\"Finds the maximum valued neighbor for a given state.\n",
        "    \n",
        "    Args:\n",
        "        state_number (int): the state to find the max neighbor for\n",
        "        state_values (float array): the respective value of each state for\n",
        "            each cell of the lake.\n",
        "    \n",
        "    Returns:\n",
        "        max_value (float): the value of the maximum neighbor.\n",
        "        policy (int): the action to take to move towards the maximum neighbor.\n",
        "    \"\"\"\n",
        "    state_x, state_y = get_state_coordinates(state_number)\n",
        "    \n",
        "    # No policy or best value yet\n",
        "    best_policy = -1\n",
        "    max_value = -np.inf\n",
        "\n",
        "    # If the cell has something other than 0, it's a terminal state.\n",
        "    if LAKE[state_y, state_x]:\n",
        "        return LAKE[state_y, state_x], best_policy\n",
        "    \n",
        "    for action in ACTION_RANGE:\n",
        "        neighbor_value = get_neighbor_value(state_x, state_y, values, action)\n",
        "        if neighbor_value > max_value:\n",
        "            max_value = neighbor_value\n",
        "            best_policy = action\n",
        "        \n",
        "    return max_value, best_policy"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IgW3YDvOsk0",
        "colab_type": "text"
      },
      "source": [
        "Now, let's write our value iteration code. We'll write a function that codes out one step of the iteration by cheacking each state and finding its maximum neighbor. The values will be reshaped so that it's in the form of the lake, but the policy will stay as a list of ints. This way, when Gym returns a state, all we need to do is look at the corresponding index in the policy list to tell our agent where to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkH-5ZnAOhPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_value(current_values):\n",
        "    \"\"\"Finds the future state values for an array of current states.\n",
        "    \n",
        "    Args:\n",
        "        current_values (int array): the value of current states.\n",
        "\n",
        "    Returns:\n",
        "        next_values (int array): The value of states based on future states.\n",
        "        next_policies (int array): The recommended action to take in a state.\n",
        "    \"\"\"\n",
        "    next_values = []\n",
        "    next_policies = []\n",
        "\n",
        "    for state in STATE_RANGE:\n",
        "        value, policy = get_max_neighbor(state, current_values)\n",
        "        next_values.append(value)\n",
        "        next_policies.append(policy)\n",
        "    \n",
        "    next_values = np.array(next_values).reshape((LAKE_HEIGHT, LAKE_WIDTH))\n",
        "    return next_values, next_policies\n",
        "\n",
        "next_values, next_policies = iterate_value(current_values)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlHmbYz4PNKF",
        "colab_type": "text"
      },
      "source": [
        "This is what our values look like after one step. Right now, it just looks like the lake. That's because we started with an array of zeros for `current_values`, and the terminal states of the lake were loaded in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yJDio3IOwqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "016f6e50-d363-459b-d606-1e85335605eb"
      },
      "source": [
        "next_values"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0],\n",
              "       [ 0, -1,  0, -1],\n",
              "       [ 0,  0,  0, -1],\n",
              "       [-1,  0,  0,  1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vNs9Y3VPe-u",
        "colab_type": "text"
      },
      "source": [
        "And this is what our policy looks like reshaped into the form of the lake. The -1's are terminal states. Right now, the agent will move left in any non-terminal state, because it sees all of those states as equal. Remember, if the gumdrop is along the leftmost side of the lake, and tries to move left, it will slip on a boulder and return to the same position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6T7fYa3PPle",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "55b60b8d-7781-4316-c080-83d6ee3d6be1"
      },
      "source": [
        "np.array(next_policies).reshape((LAKE_HEIGHT ,LAKE_WIDTH))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0],\n",
              "       [ 0, -1,  0, -1],\n",
              "       [ 0,  0,  0, -1],\n",
              "       [-1,  0,  0, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksoqUtukPq3c",
        "colab_type": "text"
      },
      "source": [
        "There's one last step to apply the Bellman Equation, the discount! We'll multiply our next states by the discount and set that to our current_values. One loop done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw2JDlSvPjVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "680611ba-0c87-4243-81da-0b97d48bd4fa"
      },
      "source": [
        "current_values = DISCOUNT * next_values\n",
        "current_values"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0. ,  0. ,  0. ,  0. ],\n",
              "       [ 0. , -0.9,  0. , -0.9],\n",
              "       [ 0. ,  0. ,  0. , -0.9],\n",
              "       [-0.9,  0. ,  0. ,  0.9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MSr1M1mP2Ix",
        "colab_type": "text"
      },
      "source": [
        "Run the below cell over and over again to see how our values change with each iteration. It should be complete after six iterations when the values no longer change. The policy will also change as the values are updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwoBgUwYPvgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c49b576c-acaa-4d29-ec1d-93fa1a59df1b"
      },
      "source": [
        "next_values, next_policies = iterate_value(current_values)\n",
        "print(\"Value\")\n",
        "print(next_values)\n",
        "print(\"Policy\")\n",
        "print(np.array(next_policies).reshape((4,4)))\n",
        "current_values = DISCOUNT * next_values"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value\n",
            "[[ 0.   0.   0.   0. ]\n",
            " [ 0.  -1.   0.  -1. ]\n",
            " [ 0.   0.   0.  -1. ]\n",
            " [-1.   0.   0.9  1. ]]\n",
            "Policy\n",
            "[[ 0  0  0  0]\n",
            " [ 0 -1  1 -1]\n",
            " [ 0  0  0 -1]\n",
            " [-1  1  2 -1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ffR6NB6Qif6",
        "colab_type": "text"
      },
      "source": [
        "Have a completed policy? Let's see it in action! We'll update our `play_game` function to instead take our list of policies. That way, we can start in a random position and still get to the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99_6_-oP6ZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cbb6799-e6ea-4cc5-9581-ec52ddb02df0"
      },
      "source": [
        "def play_game(policy):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = policy[state]  # This line is new.\n",
        "        state, _, done, _ = env.step(action)\n",
        "        env.render()\n",
        "        step += 1\n",
        "        print_state(state, done)\n",
        "\n",
        "play_game(next_policies)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Game Over!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpj-k4B3RF7L",
        "colab_type": "text"
      },
      "source": [
        "## Policy Iteration\n",
        "\n",
        "You may have noticed that the first lake was built with the parameter `is_slippery=False`. This time, we're going to switch it to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAnFBh2pQ9EW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0a4907b5-f0e1-4cd5-e4d2-d511b0f69f49"
      },
      "source": [
        "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
        "state = env.reset()\n",
        "env.render()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DP3lJ1-RPzI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f4d5426-3c9d-4aae-eb4c-d0b45464899b"
      },
      "source": [
        "play_game(next_policies)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "8 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "8 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "8 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "12 - Game Over!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9VNNcISKf_",
        "colab_type": "text"
      },
      "source": [
        "Was there a game over? There's a small chance that the gumdrop made it to the end, but it's much more likely that it accidentally slipped and fell into a hole. Oh no! We can try repeatedly testing the above code cell over and over again, but it might take a while. In fact, this is a similar rodeblock Bellman and his colleagues faced.\n",
        "\n",
        "How efficient is Value Iteration? On our modern machines, this algorithm ran fairly quickly, but back in 1960, that wasn't the case. Let's say our lake is a long straight line like this:\n",
        "\n",
        "| | | | | | | |\n",
        "|-|-|-|-|-|-|-|\n",
        "|S|F|F|F|F|F|H|\n",
        "\n",
        "This is the worse case scenario for value iteration. In each iteration, we look at every state (s) and each action per state (a), so one step of value iteration is O(s*a). In the case of our lake line, each iteration only updates one cell. In other words, the value iteration step needs to be run `s` times. In total, that's O(s<sup>2</sup>a).\n",
        "\n",
        "Back in 1960, that was computationally heavy, and so [Ronald Howard](https://en.wikipedia.org/wiki/Ronald_A._Howard) developed an alteration of Value Iteration that mildly sacrificed mathematical accuaracy for speed.\n",
        "\n",
        "Here's the strategy: it was observed that the optimal policy often converged before value iteration was complete. To take advantage of this, we'll start with random policy. When we iterate over our values, we'll use this policy instead of trying to find the maximum neighbor. This has been coded out in `find_future_values` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX-KObujSMlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_future_values(current_values, current_policies):\n",
        "    \"\"\"Finds the next set of future values based on the current policy.\"\"\"\n",
        "    next_values = []\n",
        "\n",
        "    for state in STATE_RANGE:\n",
        "        current_policy = current_policies[state]\n",
        "        state_x, state_y = get_state_coordinates(state)\n",
        "\n",
        "        # If the cell has something other than 0, it's a terminal state.\n",
        "        value = LAKE[state_y, state_x]\n",
        "        if not value:\n",
        "            value = get_neighbor_value(\n",
        "                state_x, state_y, current_values, current_policy)\n",
        "        next_values.append(value)\n",
        "\n",
        "    return np.array(next_values).reshape((LAKE_HEIGHT, LAKE_WIDTH))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX0IlRfpSdnx",
        "colab_type": "text"
      },
      "source": [
        "After we've calculated our new values, then we'll updated the policy (and not the values) based on the maximum neighbor. If there's no change in the policy, then we're done. The below is very similar to our `get_max_neighbor` function. Can you see the differences?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DqAI58dSVk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_policy(next_values):\n",
        "    \"\"\"Finds the best policy given a value mapping.\"\"\"\n",
        "    next_policies = []\n",
        "    for state in STATE_RANGE:\n",
        "        state_x, state_y = get_state_coordinates(state)\n",
        "\n",
        "        # No policy or best value yet\n",
        "        max_value = -np.inf\n",
        "        best_policy = -1\n",
        "\n",
        "        if not LAKE[state_y, state_x]:\n",
        "            for policy in ACTION_RANGE:\n",
        "                neighbor_value = get_neighbor_value(\n",
        "                    state_x, state_y, next_values, policy)\n",
        "                if neighbor_value > max_value:\n",
        "                    max_value = neighbor_value\n",
        "                    best_policy = policy\n",
        "                \n",
        "        next_policies.append(best_policy)\n",
        "    return next_policies"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaEnvG1LSnPP",
        "colab_type": "text"
      },
      "source": [
        "To complete the Policy Iteration algorithm, we'll combine the two functions above. Conceptually, we'll be alternating between updating our value function and updating our policy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UmnugxNSg8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_policy(current_values, current_policies):\n",
        "    \"\"\"Finds the future state values for an array of current states.\n",
        "    \n",
        "    Args:\n",
        "        current_values (int array): the value of current states.\n",
        "        current_policies (int array): a list where each cell is the recommended\n",
        "            action for the state matching its index.\n",
        "\n",
        "    Returns:\n",
        "        next_values (int array): The value of states based on future states.\n",
        "        next_policies (int array): The recommended action to take in a state.\n",
        "    \"\"\"\n",
        "    next_values = find_future_values(current_values, current_policies)\n",
        "    next_policies = find_best_policy(next_values)\n",
        "    return next_values, next_policies"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgXTORmdTYZ4",
        "colab_type": "text"
      },
      "source": [
        "Next, let's modify the `get_neighbor_value` function to now include the slippery ice. Remember the `P` in the Bellman Equation above? It stands for the probabilty of ending up in a new state given the current state and action taken. That is, we'll take a weighted sum of the values of all possible states based on our chances to be in those states.\n",
        "\n",
        "How does the physics of the slippery ice work? For this lake, whenever the gumdrop tries to move in a particular direction, there are three possible positions that it could end up with. It could move where it was intending to go, but it could also end up to the left or right of the direction it was facing. For instance, if it wanted to move right, it could end up on the square above or below it! This is depicted below, with the yellow squares being potential positions after attempting to move right.\n",
        "\n",
        "\n",
        "\n",
        "Each of these has an equal probability chance of happening. So since there are three outcomes, they each have about a 33% chance to happen. What happens if we slip in the direction of a boulder? No problem, we'll just end up not moving anywhere. Let's make a function to find what our possible locations could be given a policy and state coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0uQHJ5jSqrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_locations(state_x, state_y, policy):\n",
        "    left = [state_y, state_x-1]\n",
        "    down = [state_y+1, state_x]\n",
        "    right = [state_y, state_x+1]\n",
        "    up = [state_y-1, state_x]\n",
        "    directions = [left, down, right, up]\n",
        "    num_actions = len(directions)\n",
        "\n",
        "    gumdrop_right = (policy - 1) % num_actions\n",
        "    gumdrop_left = (policy + 1) % num_actions\n",
        "    locations = [gumdrop_left, policy, gumdrop_right]\n",
        "    return [directions[location] for location in locations]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yhFM24zThv6",
        "colab_type": "text"
      },
      "source": [
        "Then, we can add it to `get_neighbor_value` to find the weighted value of all the possible states the gumdrop can end up in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox4IJBD5Tdwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_neighbor_value(state_x, state_y, values, policy):\n",
        "    \"\"\"Returns the value of a state's neighbor.\n",
        "    \n",
        "    Args:\n",
        "        state_x (int): The state's horizontal position, 0 is the lake's left.\n",
        "        state_y (int): The state's vertical position, 0 is the lake's top.\n",
        "        values (float array): The current iteration's state values.\n",
        "        policy (int): Which action to check the value for.\n",
        "        \n",
        "    Returns:\n",
        "        The corresponding action's value.\n",
        "    \"\"\"\n",
        "    locations = get_locations(state_x, state_y, policy)\n",
        "    location_chance = 1.0 / len(locations)\n",
        "    total_value = 0\n",
        "\n",
        "    for location in locations:\n",
        "        check_x = location[1]\n",
        "        check_y = location[0]\n",
        "\n",
        "        is_boulder = check_y < 0 or check_y >= LAKE_HEIGHT \\\n",
        "            or check_x < 0 or check_x >= LAKE_WIDTH\n",
        "    \n",
        "        value = values[state_y, state_x]\n",
        "        if not is_boulder:\n",
        "            value = values[check_y, check_x]\n",
        "        total_value += location_chance * value\n",
        "\n",
        "    return total_value"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lbAEPtwToJe",
        "colab_type": "text"
      },
      "source": [
        "For Policy Iteration, we'll start off with a random policy if only because the Gumdrop doesn't know any better yet. We'll reset our current values while we're at it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkRiCWYfTlKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d4085778-41a5-40b3-9df9-70c1f4704e62"
      },
      "source": [
        "current_values = np.zeros_like(LAKE)\n",
        "policies = np.random.choice(ACTION_RANGE, size=STATE_SPACE)\n",
        "np.array(policies).reshape((4,4))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3, 0],\n",
              "       [0, 0, 3, 3],\n",
              "       [0, 3, 3, 1],\n",
              "       [1, 2, 1, 2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr310samTx7d",
        "colab_type": "text"
      },
      "source": [
        "As before with Value Iteration, run the cell below multiple until the policy no longer changes. It should only take 2-3 clicks compared to Value Iteration's 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wKUIhqUTs8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "15cb2f33-b88e-445e-f100-927dcf0630b1"
      },
      "source": [
        "next_values, policies = iterate_policy(current_values, policies)\n",
        "print(\"Value\")\n",
        "print(next_values)\n",
        "print(\"Policy\")\n",
        "print(np.array(policies).reshape((4,4)))\n",
        "current_values = DISCOUNT * next_values"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value\n",
            "[[ 0.  0.  0.  0.]\n",
            " [ 0. -1.  0. -1.]\n",
            " [ 0.  0.  0. -1.]\n",
            " [-1.  0.  0.  1.]]\n",
            "Policy\n",
            "[[ 0  3  0  3]\n",
            " [ 0 -1  0 -1]\n",
            " [ 3  1  0 -1]\n",
            " [-1  2  1 -1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OdSx2nkUAI0",
        "colab_type": "text"
      },
      "source": [
        "Hmm, does this work? Let's see! Run the cell below to watch the gumdrop slip its way to victory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEscHaE2T1vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7983521b-7ea9-4e31-ccf9-48784f62f381"
      },
      "source": [
        "play_game(policies)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "8 - Still Alive!\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "8 - Still Alive!\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "9 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "10 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "14 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "13 - Still Alive!\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "14 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "14 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "13 - Still Alive!\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "14 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "15 - Cocoa Time!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGpI7naiUIxN",
        "colab_type": "text"
      },
      "source": [
        "So what was the learned strategy here? The gumdrop learned to hug the left wall of boulders until it was down far enough to make a break for the exit. Instead of heading directly for it though, it took advantage of actions that did not have a hole of death in them. Patience is a virtue!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRUXw8YgXRl1",
        "colab_type": "text"
      },
      "source": [
        "So what was the learned strategy here? The gumdrop learned to hug the left wall of boulders until it was down far enough to make a break for the exit. Instead of heading directly for it though, it took advantage of actions that did not have a hole of death in them. Patience is a virtue!\n",
        "\n",
        "We promised this story was a trilogy, and yes, the next day, the gumdrop fell upon a frozen lake yet again.\n",
        "\n",
        "## Q Learning\n",
        "Value Iteration and Policy Iteration are great techniques, but what if we don't know  how big the lake is? With real world problems, not knowing how many potential states are can be a definite possibility.\n",
        "\n",
        "Enter [Chris Watkins](http://www.cs.rhul.ac.uk/~chrisw/). Inspired by how animals learn with delayed rewards, he came up with the idea of [Q Learning](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf) as an evolution of [Richard Sutton's](https://en.wikipedia.org/wiki/Richard_S._Sutton) [Temporal Difference Learning](https://en.wikipedia.org/wiki/Temporal_difference_learning). Watkins noticed that animals learn from positive and negative rewards, and that they often make mistakes in order to optimize a skill.\n",
        "\n",
        "From this emerged the idea of a Q table. In the lake case, it would look something like this.\n",
        "\n",
        "| |Left|Down|Right|Up|\n",
        "|-|-|-|-|-|\n",
        "|0| | | | |\n",
        "|1| | | | |\n",
        "|...| | | | |\n",
        "\n",
        "Here's the strategy: our agent will explore the environment. As the agent observes new states, we'll add more rows to our table. Whenever it moves from one state to the next, we'll update the cell corresponding to the old state based on the Bellman Equation. The agent doesn't need to know what the probabilities are between transitions. It'll learn the value of these as it experiments.\n",
        "\n",
        "For Q learning, this works by looking at the row that corresponds the agent's current state. Then, we'll select the action with the highest value. There are multiple ways to initialize the Q-table, but for us, we'll start with all zeros. In that case, when selecting the best action, we'll randomly select between tied max values. If we don't, the agent will favor certain actions which will limit its exploration.\n",
        "\n",
        "To be able to hande an unknown number of states, we'll initialize our q_table as one row to represent our initial state. Then, we'll make a dictionary to map new states to rows in the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrw5cwVT6n9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "46cf1c81-fcd6-4387-e9a1-f60a7a51417a"
      },
      "source": [
        "new_row = np.zeros((1, env.action_space.n))\n",
        "q_table = np.copy(new_row)\n",
        "q_map = {0: 0}\n",
        "\n",
        "def print_q(q_table, q_map):\n",
        "    print(\"mapping\")\n",
        "    print(q_map)\n",
        "    print(\"q_table\")\n",
        "    print(q_table)\n",
        "\n",
        "print_q(q_table, q_map)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mapping\n",
            "{0: 0}\n",
            "q_table\n",
            "[[0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P_FoIMiYExN",
        "colab_type": "text"
      },
      "source": [
        "Our new `get_action` function will help us read the `q_table` and find the best action.\n",
        "\n",
        "First, we'll give the agent the ability to act randomly as opposed to choosing the best known action. This gives it the ability to explore and find new situations. This is done with a random chance to act randomly. So random!\n",
        "\n",
        "When the Gumdrop chooses not to act randomly, it will instead act based on the best action recorded in the `q_table`. Numpy's [argwhere](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argwhere.html) is used to find the indexes with the maximum value in the q-table row corresponding to our current state. Since numpy is often used with higher dimensional data, each index is returned as a list of ints. Our indexes are really one dimensional since we're just looking within a single row, so we'll use [np.squeeze](https://docs.scipy.org/doc/numpy/reference/generated/numpy.squeeze.html) to remove the extra brackets. To randomly select from the indexes, we'll use [np.random.choice](https://docs.scipy.org/doc/numpy-1.14.1/reference/generated/numpy.random.choice.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQnqZOQXeZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action(q_map, q_table, state_row, random_rate):\n",
        "    \"\"\"Find max-valued actions and randomly select from them.\"\"\"\n",
        "    if random.random() < random_rate:\n",
        "        return random.randint(0, ACTION_SPACE-1)\n",
        "\n",
        "    action_values = q_table[state_row]\n",
        "    max_indexes = np.argwhere(action_values == action_values.max())\n",
        "    max_indexes = np.squeeze(max_indexes, axis=-1)\n",
        "    action = np.random.choice(max_indexes)\n",
        "    return action"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seSn-bNjYcRO",
        "colab_type": "text"
      },
      "source": [
        "Here, we'll define how the `q_table` gets updated. We'll apply the Bellman Equation as before, but since there is so much luck involved between slipping and random actions, we'll update our `q_table` as a weighted average between the `old_value` we're updating and the `future_value` based on the best action in the next state. That way, there's a little bit of memory between old and new experiences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTe58HozYXnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_q(q_table, new_state_row, reward, old_value):\n",
        "    \"\"\"Returns an updated Q-value based on the Bellman Equation.\"\"\"\n",
        "    learning_rate = .1  # Change to be between 0 and 1.\n",
        "    future_value = reward + DISCOUNT * np.max(q_table[new_state_row])\n",
        "    return old_value + learning_rate * (future_value - old_value)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsOztzWhY2Mq",
        "colab_type": "text"
      },
      "source": [
        "We'll update our `play_game` function to take our table and mapping, and at the end, we'll return any updates to them. Once we observe new states, we'll check our mapping and add then to the table if space isn't allocated for them already.\n",
        "\n",
        "Finally, for every `state` - `action` - `new-state` transition, we'll update the cell in `q_table` that corresponds to the `state` and `action` with the Bellman Equation.\n",
        "\n",
        "There's a little secret to solving this lake problem, and that's to have a small negative reward when moving between states. Otherwise, the grumdrop will become too afraid of slipping in a death hole to explore out of what is thought to be safe positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBW_mDhYkhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(q_table, q_map, random_rate, render=False):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_row = q_map[state]\n",
        "        action = get_action(q_map, q_table, state_row, random_rate)\n",
        "        new_state, _, done, _ = env.step(action)\n",
        "\n",
        "        #Add new state to table and mapping if it isn't there already.\n",
        "        if new_state not in q_map:\n",
        "            q_map[new_state] = len(q_table)\n",
        "            q_table = np.append(q_table, new_row, axis=0)\n",
        "        new_state_row = q_map[new_state]\n",
        "\n",
        "        reward = -.01  #Encourage exploration.\n",
        "        if done:\n",
        "            reward = 1 if new_state == 15 else -1\n",
        "        current_q = q_table[state_row, action]\n",
        "        q_table[state_row, action] = update_q(\n",
        "            q_table, new_state_row, reward, current_q)\n",
        "\n",
        "        step += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            print_state(new_state, done)\n",
        "        state = new_state\n",
        "        \n",
        "    return q_table, q_map"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN8_TF2UZJ5L",
        "colab_type": "text"
      },
      "source": [
        "Ok, time to shine, gumdrop emoji! Let's do one simulation and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXm_UdOmZc6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run to refresh the q_table.\n",
        "random_rate = 1\n",
        "q_table = np.copy(new_row)\n",
        "q_map = {0: 0}"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHdtMjURaMQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh1aDSKtZdJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efefdde1-00e7-4860-a7b4-e73780f11e82"
      },
      "source": [
        "q_table, q_map = play_game(q_table, q_map, random_rate, render=True)\n",
        "print_q(q_table, q_map)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "1 - Still Alive!\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2 - Still Alive!\n",
            "  (Left)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "1 - Still Alive!\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "5 - Game Over!\n",
            "mapping\n",
            "{0: 0, 1: 1, 2: 2, 4: 3, 5: 4}\n",
            "q_table\n",
            "[[-0.001  -0.0019 -0.001  -0.001 ]\n",
            " [ 0.     -0.001   0.     -0.001 ]\n",
            " [-0.001   0.      0.      0.    ]\n",
            " [ 0.      0.      0.     -0.1   ]\n",
            " [ 0.      0.      0.      0.    ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njXRlSsFZpD-",
        "colab_type": "text"
      },
      "source": [
        "Unless the gumdrop was incredibly lucky, chances were, it fell in some death water. Q-learning is markedly different from Value Iteration or Policy Iteration in that it attempts to simulate how an animal learns in unknown situations. Since the layout of the lake is unknown to the Gumdrop, it doesn't know which states are death holes, and which ones are safe. Because of this, it's going to make many mistakes before it can start making successes.\n",
        "\n",
        "Feel free to run the above cell multiple times to see how the gumdrop steps through trial and error. When you're ready, run the below cell to have the gumdrop play 1000 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUAcQLkpY6Ep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "7447ef6c-d81b-47fa-deb9-c00da8b8ab37"
      },
      "source": [
        "for _ in range(1000):\n",
        "    q_table, q_map = play_game(q_table, q_map, random_rate)\n",
        "    random_rate = random_rate * .99\n",
        "print_q(q_table, q_map)\n",
        "random_rate"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mapping\n",
            "{0: 0, 1: 1, 2: 2, 4: 3, 5: 4, 3: 5, 7: 6, 8: 7, 6: 8, 9: 9, 10: 10, 14: 11, 15: 12, 11: 13, 12: 14, 13: 15}\n",
            "q_table\n",
            "[[-0.07039558 -0.2424438  -0.22734814 -0.27284765]\n",
            " [-0.52271395 -0.40218021 -0.40984035 -0.13088649]\n",
            " [-0.30051885 -0.19684595 -0.29238059 -0.29142778]\n",
            " [-0.06181289 -0.50976797 -0.24092395 -0.32489924]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.34337853 -0.61684068 -0.44824642 -0.17603579]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.23016715 -0.3733103  -0.3555287   0.03563979]\n",
            " [-0.3838617  -0.59340627 -0.62112779 -0.62295569]\n",
            " [-0.42643371  0.12325753 -0.34590636 -0.20870362]\n",
            " [ 0.24103398 -0.3024524  -0.26926176 -0.2635058 ]\n",
            " [ 0.20890509  0.66340883  0.27594773  0.18110908]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.19673678 -0.04399625  0.3078323  -0.45281699]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.317124741065784e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVoVRJ1IZudj",
        "colab_type": "text"
      },
      "source": [
        "Cats have nine lives, our Gumdrop lived a thousand! Moment of truth. Can it get out of the lake now that it matters?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcBOsK1-ZUpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eea3ff57-6f60-4d95-dff3-c013649f9de7"
      },
      "source": [
        "q_table, q_map = play_game(q_table, q_map, 0, render=True)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "4 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "8 - Still Alive!\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "9 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "10 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "6 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "10 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "6 - Still Alive!\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2 - Still Alive!\n",
            "  (Down)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3 - Still Alive!\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3 - Still Alive!\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3 - Still Alive!\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3 - Still Alive!\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3 - Still Alive!\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3 - Still Alive!\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "6 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "10 - Still Alive!\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "14 - Still Alive!\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "15 - Cocoa Time!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY1kC6jmZzCJ",
        "colab_type": "text"
      },
      "source": [
        "Third time's the charm!\n",
        "\n",
        "Each of these techniques has their pros and cons. For instance, while Value Iteration is the mathematically correct solution, it's not as time efficient at Policy Iteration or as flexible as Q-Learning.\n",
        "\n",
        "| |Value Iteration|Policy Iteration|Q Tables|\n",
        "|-|-|-|-|\n",
        "|Avoids locally optimal routes|✓|x|x|\n",
        "|On-policy (greedy)|✓|✓|x|\n",
        "|Model Free|x|x|✓|\n",
        "|Most time efficient|x|✓|x|\n",
        "\n",
        "Congratulations on making it through to the end. Now if you ever fall on a Frozen Lake, you'll have may different ways to calculate your survival. The gumdrop thanks you!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMvbeyEwZ256",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}